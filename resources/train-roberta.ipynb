{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9510375,"sourceType":"datasetVersion","datasetId":5788903}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ntrain_set = pd.read_csv(\"datasets/train.csv\", encoding = 'latin-1')\ntargets_list = (train_set[\"target\"].unique()).tolist()\ntrain_set['label'] = train_set['target'].apply(lambda t: targets_list.index(t) if t in targets_list else -1)","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(targets_list)","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"['academic interests', 'arts and culture', 'automotives', 'books and literature', 'business and finance', 'careers', 'family and relationships', 'food and drinks', 'health', 'healthy living', 'hobbies and interests', 'home and garden', 'movies', 'music and audio', 'news and politics', 'personal finance', 'pets', 'pharmaceuticals, conditions, and symptoms', 'real estate', 'shopping', 'sports', 'style and fashion', 'technology and computing', 'television', 'travel', 'video gaming']\n"}]},{"cell_type":"code","source":"print(sorted(targets_list))","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"['academic interests', 'arts and culture', 'automotives', 'books and literature', 'business and finance', 'careers', 'family and relationships', 'food and drinks', 'health', 'healthy living', 'hobbies and interests', 'home and garden', 'movies', 'music and audio', 'news and politics', 'personal finance', 'pets', 'pharmaceuticals, conditions, and symptoms', 'real estate', 'shopping', 'sports', 'style and fashion', 'technology and computing', 'television', 'travel', 'video gaming']\n"}]},{"cell_type":"code","source":"len(targets_list)","metadata":{},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{}}]},{"cell_type":"code","source":"!pip install nltk","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.9.1)\n\nRequirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (8.1.7)\n\nRequirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (1.4.2)\n\nRequirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (2024.9.11)\n\nRequirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (4.66.5)\n"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import random\nimport nltk\nfrom nltk.corpus import wordnet as wn\nimport re\n# Make sure to download the required NLTK data\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\ndef get_synonym(word):\n    synonyms = wn.synsets(word)\n    if synonyms:\n        words = set(chain.from_iterable([syn.lemma_names() for syn in synonyms]))\n        words.discard(word)  # Avoid returning the same word\n        if words:\n            return random.choice(list(words))\n    return word\n\ndef augment_text(text):\n    words = text.split()\n    augmented_text = []\n    for word in words:\n        if random.random() < 0.3:  # 30% chance of replacing a word\n            augmented_text.append(get_synonym(word))\n        else:\n            augmented_text.append(word)\n    return ' '.join(augmented_text)\n\ndef augment_dataframe(df, fraction):\n    to_augment = df.sample(frac=fraction).index\n    df['text'] = df['text'].apply(lambda x: augment_text(x) if x in to_augment else x)\n    return df\n\n\nclass TextCleaner():\n    def __init__(self):\n        pass\n    \n    def clean_text(self, text):\n        text = (str(text)).lower()\n        text = re.sub(r'<.*?>', '', text)\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n\ncleaner = TextCleaner()","metadata":{},"execution_count":7,"outputs":[{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package wordnet to\n\n[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n\n[nltk_data]   Package wordnet is already up-to-date!\n\n[nltk_data] Downloading package omw-1.4 to\n\n[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n\n[nltk_data]   Package omw-1.4 is already up-to-date!\n"}]},{"cell_type":"markdown","source":"# Preparing Data","metadata":{}},{"cell_type":"code","source":"train_set['text'] = train_set['text'].apply(cleaner.clean_text)","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_shuffled = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n\nselected_samples = pd.DataFrame()\n\n# Get unique classes\nunique_classes = df_shuffled['target'].unique()\n\n# Step 3: Ensure each class has at least 15,000 samples\nfor class_name in unique_classes:\n    class_samples = df_shuffled[df_shuffled['target'] == class_name]\n    \n    # Check if there are enough samples for the class\n    if len(class_samples) >= 16000:\n        selected_samples = pd.concat([selected_samples, class_samples.sample(n=16000, random_state=42)])\n\n# Step 4: Calculate remaining samples needed\nremaining_samples_needed = 675000 - len(selected_samples)\n\n# Step 5: Select the remaining samples randomly from the rest of the dataframe\nremaining_df = df_shuffled[~df_shuffled.index.isin(selected_samples.index)]\nadditional_samples = remaining_df.sample(n=remaining_samples_needed, random_state=42)\n\n# Step 6: Combine selected samples and additional samples\ntrain_df = pd.concat([selected_samples, additional_samples]).sample(frac=1, random_state=42)\n\n# Step 7: Split the remaining data into validation and evaluation datasets\nremaining_df = remaining_df[~remaining_df.index.isin(train_df .index)]\nval_df = remaining_df.sample(frac=0.95, random_state=42)\neval_df = remaining_df.drop(val_df.index)\n\n# Optional: Reset indices if needed\ntrain_df .reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\neval_df.reset_index(drop=True, inplace=True)\n\nlen(train_df)","metadata":{},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":["675000"]},"metadata":{}}]},{"cell_type":"code","source":"len(train_df), len(val_df), len(eval_df)","metadata":{},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":["(675000, 21401, 1126)"]},"metadata":{}}]},{"cell_type":"code","source":"train_df = augment_dataframe(train_df, fraction = 0.30)","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.sample(frac=1, random_state=43).reset_index(drop=True)","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(train_df.groupby([\"target\"]).count())\ndf","metadata":{},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>Word Count</th>\n","      <th>label</th>\n","    </tr>\n","    <tr>\n","      <th>target</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>academic interests</th>\n","      <td>55208</td>\n","      <td>55208</td>\n","      <td>55208</td>\n","    </tr>\n","    <tr>\n","      <th>arts and culture</th>\n","      <td>25516</td>\n","      <td>25516</td>\n","      <td>25516</td>\n","    </tr>\n","    <tr>\n","      <th>automotives</th>\n","      <td>27074</td>\n","      <td>27074</td>\n","      <td>27074</td>\n","    </tr>\n","    <tr>\n","      <th>books and literature</th>\n","      <td>38493</td>\n","      <td>38493</td>\n","      <td>38493</td>\n","    </tr>\n","    <tr>\n","      <th>business and finance</th>\n","      <td>27088</td>\n","      <td>27088</td>\n","      <td>27088</td>\n","    </tr>\n","    <tr>\n","      <th>careers</th>\n","      <td>29322</td>\n","      <td>29322</td>\n","      <td>29322</td>\n","    </tr>\n","    <tr>\n","      <th>family and relationships</th>\n","      <td>27928</td>\n","      <td>27928</td>\n","      <td>27928</td>\n","    </tr>\n","    <tr>\n","      <th>food and drinks</th>\n","      <td>22848</td>\n","      <td>22848</td>\n","      <td>22848</td>\n","    </tr>\n","    <tr>\n","      <th>health</th>\n","      <td>18706</td>\n","      <td>18706</td>\n","      <td>18706</td>\n","    </tr>\n","    <tr>\n","      <th>healthy living</th>\n","      <td>29744</td>\n","      <td>29744</td>\n","      <td>29744</td>\n","    </tr>\n","    <tr>\n","      <th>hobbies and interests</th>\n","      <td>23310</td>\n","      <td>23310</td>\n","      <td>23310</td>\n","    </tr>\n","    <tr>\n","      <th>home and garden</th>\n","      <td>22470</td>\n","      <td>22470</td>\n","      <td>22470</td>\n","    </tr>\n","    <tr>\n","      <th>movies</th>\n","      <td>21332</td>\n","      <td>21332</td>\n","      <td>21332</td>\n","    </tr>\n","    <tr>\n","      <th>music and audio</th>\n","      <td>20239</td>\n","      <td>20239</td>\n","      <td>20239</td>\n","    </tr>\n","    <tr>\n","      <th>news and politics</th>\n","      <td>29027</td>\n","      <td>29027</td>\n","      <td>29027</td>\n","    </tr>\n","    <tr>\n","      <th>personal finance</th>\n","      <td>20729</td>\n","      <td>20729</td>\n","      <td>20729</td>\n","    </tr>\n","    <tr>\n","      <th>pets</th>\n","      <td>23494</td>\n","      <td>23494</td>\n","      <td>23494</td>\n","    </tr>\n","    <tr>\n","      <th>pharmaceuticals, conditions, and symptoms</th>\n","      <td>25703</td>\n","      <td>25703</td>\n","      <td>25703</td>\n","    </tr>\n","    <tr>\n","      <th>real estate</th>\n","      <td>23210</td>\n","      <td>23210</td>\n","      <td>23210</td>\n","    </tr>\n","    <tr>\n","      <th>shopping</th>\n","      <td>28399</td>\n","      <td>28399</td>\n","      <td>28399</td>\n","    </tr>\n","    <tr>\n","      <th>sports</th>\n","      <td>23846</td>\n","      <td>23846</td>\n","      <td>23846</td>\n","    </tr>\n","    <tr>\n","      <th>style and fashion</th>\n","      <td>28093</td>\n","      <td>28093</td>\n","      <td>28093</td>\n","    </tr>\n","    <tr>\n","      <th>technology and computing</th>\n","      <td>20370</td>\n","      <td>20370</td>\n","      <td>20370</td>\n","    </tr>\n","    <tr>\n","      <th>television</th>\n","      <td>19228</td>\n","      <td>19228</td>\n","      <td>19228</td>\n","    </tr>\n","    <tr>\n","      <th>travel</th>\n","      <td>21210</td>\n","      <td>21210</td>\n","      <td>21210</td>\n","    </tr>\n","    <tr>\n","      <th>video gaming</th>\n","      <td>22413</td>\n","      <td>22413</td>\n","      <td>22413</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            text  Word Count  label\n","target                                                             \n","academic interests                         55208       55208  55208\n","arts and culture                           25516       25516  25516\n","automotives                                27074       27074  27074\n","books and literature                       38493       38493  38493\n","business and finance                       27088       27088  27088\n","careers                                    29322       29322  29322\n","family and relationships                   27928       27928  27928\n","food and drinks                            22848       22848  22848\n","health                                     18706       18706  18706\n","healthy living                             29744       29744  29744\n","hobbies and interests                      23310       23310  23310\n","home and garden                            22470       22470  22470\n","movies                                     21332       21332  21332\n","music and audio                            20239       20239  20239\n","news and politics                          29027       29027  29027\n","personal finance                           20729       20729  20729\n","pets                                       23494       23494  23494\n","pharmaceuticals, conditions, and symptoms  25703       25703  25703\n","real estate                                23210       23210  23210\n","shopping                                   28399       28399  28399\n","sports                                     23846       23846  23846\n","style and fashion                          28093       28093  28093\n","technology and computing                   20370       20370  20370\n","television                                 19228       19228  19228\n","travel                                     21210       21210  21210\n","video gaming                               22413       22413  22413"]},"metadata":{}}]},{"cell_type":"code","source":"train_data = Dataset.from_pandas(train_df[['text', 'label']])\nval_data = Dataset.from_pandas(val_df[['text', 'label']])\ntrain_data, val_data","metadata":{},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 675000\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 21401\n"," }))"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"#<your wandb key\n!pip install huggingface_hub\nfrom huggingface_hub import login\n\napi_token = '<API_Token'\nlogin(api_token)","metadata":{"scrolled":true},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: huggingface_hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.1)\n\nRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n\nRequirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\n\nRequirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (24.1)\n\nRequirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n\nRequirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n\nRequirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n\nRequirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n\nToken is valid (permission: fineGrained).\n\nYour token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token\n\nLogin successful\n"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, RobertaForSequenceClassification\n\nmodel_name = \"pilotj/roberta-base-pretrained-v1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512)\n\nencoded_train_data = train_data.map(preprocess_function, batched=True, batch_size=256)\nencoded_val_data = val_data.map(preprocess_function, batched=True, batch_size=256)\nencoded_train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nencoded_val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])","metadata":{},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"565bf2d51f8c45acb174de909202adee","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/675000 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25e6c5cec9304f8f96a99a73ac34d504","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21401 [00:00<?, ? examples/s]"]},"metadata":{}}]},{"cell_type":"code","source":"len(encoded_train_data), len(encoded_val_data)","metadata":{},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":["(675000, 21401)"]},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\ndef compute_metrics(p):\n    # Get predictions and true labels\n    preds = np.argmax(p.predictions, axis=1)  # p.predictions are logits, take argmax to get class predictions\n    labels = p.label_ids  # True labels\n\n    f1_macro = f1_score(labels, preds, average='macro')  # or 'macro' / 'micro'\n    f1_w = f1_score(labels, preds, average='weighted')\n    # Calculate other optional metrics if needed\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n\n    # Return metrics as a dictionary\n    return {\n        'accuracy': accuracy,\n        'f1_macro': f1_macro,\n        'f1_w': f1_w,\n        'precision': precision,\n        'recall': recall\n    }\n","metadata":{},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# import torch\n# torch.cuda.empty_cache()","metadata":{},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='results',\n    overwrite_output_dir=True,\n    learning_rate=2e-5,\n    save_total_limit=3,\n    push_to_hub=True,\n    hub_model_id=\"pilotj/roberta-base-v1\",\n    hub_strategy=\"checkpoint\",\n    save_steps=1000,\n    eval_steps=500,\n    save_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    load_best_model_at_end=True,\n    per_device_train_batch_size=128,\n    per_device_eval_batch_size=64,\n    gradient_accumulation_steps=2,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    fp16 = True\n)","metadata":{},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_data,\n    eval_dataset=encoded_val_data,\n    compute_metrics = compute_metrics\n)","metadata":{},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-09-29T04:59:28.803689Z","iopub.status.busy":"2024-09-29T04:59:28.803301Z","iopub.status.idle":"2024-09-29T04:59:28.808004Z","shell.execute_reply":"2024-09-29T04:59:28.807070Z","shell.execute_reply.started":"2024-09-29T04:59:28.803645Z"}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\ntrainer.train()","metadata":{},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5274' max='5274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5274/5274 1:38:10, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Macro</th>\n","      <th>F1 W</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.393200</td>\n","      <td>0.413825</td>\n","      <td>0.880286</td>\n","      <td>0.850493</td>\n","      <td>0.881589</td>\n","      <td>0.884721</td>\n","      <td>0.880286</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.399700</td>\n","      <td>0.409719</td>\n","      <td>0.880940</td>\n","      <td>0.849948</td>\n","      <td>0.882442</td>\n","      <td>0.886069</td>\n","      <td>0.880940</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.399700</td>\n","      <td>0.412584</td>\n","      <td>0.881781</td>\n","      <td>0.851393</td>\n","      <td>0.883361</td>\n","      <td>0.887383</td>\n","      <td>0.881781</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.390700</td>\n","      <td>0.398750</td>\n","      <td>0.884351</td>\n","      <td>0.854420</td>\n","      <td>0.885605</td>\n","      <td>0.888745</td>\n","      <td>0.884351</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.388100</td>\n","      <td>0.395556</td>\n","      <td>0.886174</td>\n","      <td>0.854916</td>\n","      <td>0.887140</td>\n","      <td>0.890059</td>\n","      <td>0.886174</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.355800</td>\n","      <td>0.397092</td>\n","      <td>0.886267</td>\n","      <td>0.857005</td>\n","      <td>0.887421</td>\n","      <td>0.890191</td>\n","      <td>0.886267</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.352600</td>\n","      <td>0.399867</td>\n","      <td>0.885239</td>\n","      <td>0.855845</td>\n","      <td>0.886699</td>\n","      <td>0.890165</td>\n","      <td>0.885239</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.343500</td>\n","      <td>0.399148</td>\n","      <td>0.885753</td>\n","      <td>0.856547</td>\n","      <td>0.887043</td>\n","      <td>0.890260</td>\n","      <td>0.885753</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.342800</td>\n","      <td>0.392920</td>\n","      <td>0.885940</td>\n","      <td>0.857234</td>\n","      <td>0.887125</td>\n","      <td>0.890092</td>\n","      <td>0.885940</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.339200</td>\n","      <td>0.392029</td>\n","      <td>0.886734</td>\n","      <td>0.857584</td>\n","      <td>0.887973</td>\n","      <td>0.890896</td>\n","      <td>0.886734</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5274, training_loss=0.36853346320142516, metrics={'train_runtime': 5892.0295, 'train_samples_per_second': 229.123, 'train_steps_per_second': 0.895, 'total_flos': 3.552764654592e+17, 'train_loss': 0.36853346320142516, 'epoch': 2.0})"]},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"pilotj/roberta-base-v1\")\ntokenizer.push_to_hub(\"pilotj/roberta-base-v1\")\ntrainer.push_to_hub(\"pilotj/roberta-base-v1\")","metadata":{},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ba41c0f0d954231be2ac215d1ea486b","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2bee88e9c3241ddb3dbc4a603a2be16","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1727893591.ip-10-192-11-81.1134.3:   0%|          | 0.00/13.9k [00:00<?, ?B/s]"]},"metadata":{}},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/pilotj/roberta-base-v1/commit/dcf1cdea07d3935847d86eef5efb2fb5a9431532', commit_message='pilotj/roberta-base-v1', commit_description='', oid='dcf1cdea07d3935847d86eef5efb2fb5a9431532', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pilotj/roberta-base-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='pilotj/roberta-base-v1'), pr_revision=None, pr_num=None)"]},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model\neval_data = Dataset.from_pandas(eval_df[['text', 'label']])\nencoded_eval_data = eval_data.map(preprocess_function, batched=True, batch_size=256)\nencoded_eval_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntrainer.evaluate(encoded_eval_data)","metadata":{},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59e5e5c69b6b4db2b5d1fe255727084e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1126 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18/18 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.3762845993041992,\n"," 'eval_accuracy': 0.8863232682060391,\n"," 'eval_f1_macro': 0.8644397972414607,\n"," 'eval_f1_w': 0.8869261992122593,\n"," 'eval_precision': 0.8908920626537057,\n"," 'eval_recall': 0.8863232682060391,\n"," 'eval_runtime': 1.3749,\n"," 'eval_samples_per_second': 818.941,\n"," 'eval_steps_per_second': 13.091,\n"," 'epoch': 2.0}"]},"metadata":{}}]}]}